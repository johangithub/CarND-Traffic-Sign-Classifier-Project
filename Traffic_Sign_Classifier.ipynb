{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Term 1: Deep Learning\n",
    "\n",
    "## Project 2: Build a Traffic Sign Recognition Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 1: Dataset Summary & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of validation examples = 4410\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n",
      "Count of each class in list\n",
      "[(0, 180), (1, 1980), (2, 2010), (3, 1260), (4, 1770), (5, 1650), (6, 360), (7, 1290), (8, 1260), (9, 1320), (10, 1800), (11, 1170), (12, 1890), (13, 1920), (14, 690), (15, 540), (16, 360), (17, 990), (18, 1080), (19, 180), (20, 300), (21, 270), (22, 330), (23, 450), (24, 240), (25, 1350), (26, 540), (27, 210), (28, 480), (29, 240), (30, 390), (31, 690), (32, 210), (33, 599), (34, 360), (35, 1080), (36, 330), (37, 180), (38, 1860), (39, 270), (40, 300), (41, 210), (42, 210)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac4e4a81d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "# Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = './train.p'\n",
    "validation_file='./valid.p'\n",
    "testing_file = './test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "\n",
    "X_train = X_train.astype(float)\n",
    "X_train = (X_train - 128)/128\n",
    "# %%\n",
    "\n",
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "# Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = X_train.shape[1:]\n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "\n",
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "#%matplotlib inline\n",
    "\n",
    "#index = random.randint(0, len(X_train))\n",
    "#image = X_train[index].squeeze()\n",
    "#show one example\n",
    "#plt.figure(figsize=(1,1))\n",
    "#plt.imshow(image)\n",
    "\n",
    "#Count of each class\n",
    "count = np.bincount(y_train)\n",
    "ii = np.nonzero(count)[0]\n",
    "freq = zip(ii, count[ii])\n",
    "\n",
    "print('Count of each class in list')\n",
    "print([i for i in freq])\n",
    "plt.bar(range(len(ii)),count[ii])\n",
    "plt.title('Distribution of signs in the training data')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Signs')\n",
    "plt.savefig('writeup_plots/distribution.png')\n",
    "\n",
    "plt.show()\n",
    "#print('Class: ',y_train[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def VGG_small(x):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    strides = [1,1,1,1]\n",
    "    # Conv Layer 1_1. Input = 32x32x3. Output = 32x32x64.\n",
    "    F_W = tf.Variable(tf.truncated_normal([3,3,3,32], mean=mu, stddev=sigma))\n",
    "    F_b = tf.Variable(tf.zeros(32))\n",
    "\n",
    "\n",
    "    conv1_1 = tf.nn.conv2d(x, F_W, strides, 'SAME') + F_b\n",
    "    conv1_1 = tf.nn.relu(conv1_1)\n",
    "    \n",
    "    # Conv Layer 1_2: Input 32x32x64. Output = 32x32x64\n",
    "    F_W2 = tf.Variable(tf.truncated_normal([3,3,32,32], mean=mu, stddev=sigma))\n",
    "    F_b2 = tf.Variable(tf.zeros(32))\n",
    "\n",
    "    conv1_2 = tf.nn.conv2d(conv1_1, F_W2, strides, 'SAME') + F_b2\n",
    "    conv1_2 = tf.nn.relu(conv1_2)\n",
    "    \n",
    "    # Pooling 1. Input = 32x32x64. Output = 16x16x64.\n",
    "    pool1 = tf.nn.max_pool(conv1_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    # Conv Layer 2_1. Input = 16x16x64. Output = 16x16x128.\n",
    "    F_W3 = tf.Variable(tf.truncated_normal([3,3,32,64], mean=mu, stddev=sigma))\n",
    "    F_b3 = tf.Variable(tf.zeros(64))\n",
    "\n",
    "    conv2_1 = tf.nn.conv2d(pool1, F_W3, strides, 'SAME') + F_b3\n",
    "    conv2_1 = tf.nn.relu(conv2_1)\n",
    "    \n",
    "    # Conv Layer 2_2. Input = 16x16x128. Output = 16x16x128.\n",
    "    F_W4 = tf.Variable(tf.truncated_normal([3,3,64,64], mean=mu, stddev=sigma))\n",
    "    F_b4 = tf.Variable(tf.zeros(64))\n",
    "\n",
    "    conv2_2 = tf.nn.conv2d(conv2_1, F_W4, strides, 'SAME') + F_b4\n",
    "    conv2_2 = tf.nn.relu(conv2_2)\n",
    "    \n",
    "    # Pooling 2. Input = 16x16x128. Output = 8x8x128.\n",
    "    pool2 = tf.nn.max_pool(conv2_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    \"\"\"\n",
    "    # Conv Layer 3_1. Input = 8x8x128. Output = 8x8x256.\n",
    "    F_W5 = tf.Variable(tf.truncated_normal([3,3,128,256], mean=mu, stddev=sigma))\n",
    "    F_b5 = tf.Variable(tf.zeros(256))\n",
    "\n",
    "    conv3_1 = tf.nn.conv2d(pool2, F_W5, strides, 'SAME') + F_b5\n",
    "    conv3_1 = tf.nn.relu(conv3_1)\n",
    "    \n",
    "    # Conv Layer 3_2. Input = 8x8x256. Output = 8x8x256.\n",
    "    F_W6 = tf.Variable(tf.truncated_normal([3,3,256,256], mean=mu, stddev=sigma))\n",
    "    F_b6 = tf.Variable(tf.zeros(256))\n",
    "\n",
    "    conv3_2 = tf.nn.conv2d(conv3_1, F_W6, strides, 'SAME') + F_b6\n",
    "    conv3_2 = tf.nn.relu(conv3_2)\n",
    "    \n",
    "    # Pooling 3. Input = 8x8x256. Output = 4x4x256.\n",
    "    pool3 = tf.nn.max_pool(conv3_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten. Input = 8x8x128 = 8192. Output = 128\n",
    "    F_W7 = tf.Variable(tf.truncated_normal([4096, 128], mean=mu, stddev=sigma))\n",
    "    F_b7 = tf.Variable(tf.zeros(128))\n",
    "    fc1 = tf.reshape(pool2, [-1, F_W7.get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, F_W7), F_b7)\n",
    "    #fc1 = tf.contrib.layers.batch_norm(fc1, center=True, scale=True, is_training=phase, scope='bn1')\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    # Fully Connected Layer 2. Input = 128. Output = 43\n",
    "    F_W8 = tf.Variable(tf.truncated_normal([128, n_classes], mean=mu, stddev=sigma))\n",
    "    F_b8 = tf.Variable(tf.zeros(n_classes))\n",
    "    fc2 = tf.add(tf.matmul(fc1, F_W8), F_b8)\n",
    "    #fc2 = tf.contrib.layers.batch_norm(fc2, center=True, scale=True, is_training=phase, scope='bn2')\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Fully Connected Layer 3. Input = 43. Output = 43    \n",
    "    F_W9 = tf.Variable(tf.truncated_normal([n_classes, n_classes], mean=mu, stddev=sigma))\n",
    "    F_b9 = tf.Variable(tf.zeros(n_classes))\n",
    "    fc3 = tf.add(tf.matmul(fc2, F_W9), F_b9)\n",
    "    fc3 = tf.nn.relu(fc3)\n",
    "    fc3 = tf.nn.dropout(fc3, keep_prob)\n",
    "    \"\"\"\n",
    "    \n",
    "    F_W10 = tf.Variable(tf.truncated_normal([n_classes, n_classes], mean=mu, stddev=sigma))\n",
    "    F_b10 = tf.Variable(tf.zeros(n_classes))\n",
    "    logits = tf.add(tf.matmul(fc2, F_W10), F_b10)\n",
    "    \n",
    "    return logits\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "### Train your model here.\n",
    "### Calculate and report the accuracy on the training and validation set.\n",
    "### Once a final model architecture is selected, \n",
    "### the accuracy on the test set should be calculated and reported as well.\n",
    "### Feel free to use as many code cells as needed.\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "#phase = tf.placeholder(tf.bool)\n",
    "\n",
    "learning_rate = 5e-4\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "logits = VGG_small(x)\n",
    "\n",
    "# For decaying learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "decaying_rate = tf.train.exponential_decay(learning_rate, global_step, 100000, 0.94, staircase=True)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer = tf.train.AdamOptimizer(decaying_rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 0 ... 4.86s passed\n",
      "Validation Accuracy = 0.305\n",
      "\n",
      "EPOCH 1 ... 9.69s passed\n",
      "Validation Accuracy = 0.439\n",
      "\n",
      "EPOCH 2 ... 14.54s passed\n",
      "Validation Accuracy = 0.567\n",
      "\n",
      "EPOCH 3 ... 19.35s passed\n",
      "Validation Accuracy = 0.697\n",
      "\n",
      "EPOCH 4 ... 24.12s passed\n",
      "Validation Accuracy = 0.811\n",
      "\n",
      "EPOCH 5 ... 28.91s passed\n",
      "Validation Accuracy = 0.825\n",
      "\n",
      "EPOCH 6 ... 33.87s passed\n",
      "Validation Accuracy = 0.861\n",
      "\n",
      "EPOCH 7 ... 38.74s passed\n",
      "Validation Accuracy = 0.868\n",
      "\n",
      "EPOCH 8 ... 43.59s passed\n",
      "Validation Accuracy = 0.867\n",
      "\n",
      "EPOCH 9 ... 48.39s passed\n",
      "Validation Accuracy = 0.898\n",
      "\n",
      "EPOCH 10 ... 53.26s passed\n",
      "Validation Accuracy = 0.909\n",
      "\n",
      "EPOCH 11 ... 58.08s passed\n",
      "Validation Accuracy = 0.901\n",
      "\n",
      "EPOCH 12 ... 62.90s passed\n",
      "Validation Accuracy = 0.911\n",
      "\n",
      "EPOCH 13 ... 67.73s passed\n",
      "Validation Accuracy = 0.919\n",
      "\n",
      "EPOCH 14 ... 72.54s passed\n",
      "Validation Accuracy = 0.927\n",
      "\n",
      "EPOCH 15 ... 77.40s passed\n",
      "Validation Accuracy = 0.921\n",
      "\n",
      "EPOCH 16 ... 82.24s passed\n",
      "Validation Accuracy = 0.936\n",
      "\n",
      "EPOCH 17 ... 87.16s passed\n",
      "Validation Accuracy = 0.936\n",
      "\n",
      "EPOCH 18 ... 91.93s passed\n",
      "Validation Accuracy = 0.928\n",
      "\n",
      "EPOCH 19 ... 96.77s passed\n",
      "Validation Accuracy = 0.917\n",
      "\n",
      "EPOCH 20 ... 101.60s passed\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "EPOCH 21 ... 106.40s passed\n",
      "Validation Accuracy = 0.937\n",
      "\n",
      "EPOCH 22 ... 111.20s passed\n",
      "Validation Accuracy = 0.943\n",
      "\n",
      "EPOCH 23 ... 115.99s passed\n",
      "Validation Accuracy = 0.940\n",
      "\n",
      "EPOCH 24 ... 120.75s passed\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "EPOCH 25 ... 125.52s passed\n",
      "Validation Accuracy = 0.940\n",
      "\n",
      "EPOCH 26 ... 130.28s passed\n",
      "Validation Accuracy = 0.949\n",
      "\n",
      "EPOCH 27 ... 135.05s passed\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "EPOCH 28 ... 139.85s passed\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "EPOCH 29 ... 144.64s passed\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "EPOCH 30 ... 149.44s passed\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "EPOCH 31 ... 154.25s passed\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "EPOCH 32 ... 159.00s passed\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "EPOCH 33 ... 163.75s passed\n",
      "Validation Accuracy = 0.966\n",
      "\n",
      "EPOCH 34 ... 168.49s passed\n",
      "Validation Accuracy = 0.961\n",
      "\n",
      "EPOCH 35 ... 173.32s passed\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "EPOCH 36 ... 178.16s passed\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "EPOCH 37 ... 182.91s passed\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "EPOCH 38 ... 187.71s passed\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "EPOCH 39 ... 192.52s passed\n",
      "Validation Accuracy = 0.923\n",
      "\n",
      "EPOCH 40 ... 197.34s passed\n",
      "Validation Accuracy = 0.959\n",
      "\n",
      "EPOCH 41 ... 202.20s passed\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "EPOCH 42 ... 207.02s passed\n",
      "Validation Accuracy = 0.961\n",
      "\n",
      "EPOCH 43 ... 211.92s passed\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "EPOCH 44 ... 216.74s passed\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "EPOCH 45 ... 221.62s passed\n",
      "Validation Accuracy = 0.964\n",
      "\n",
      "EPOCH 46 ... 226.55s passed\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "EPOCH 47 ... 231.48s passed\n",
      "Validation Accuracy = 0.935\n",
      "\n",
      "EPOCH 48 ... 236.39s passed\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "EPOCH 49 ... 241.23s passed\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "EPOCH 50 ... 246.21s passed\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "EPOCH 51 ... 251.13s passed\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "EPOCH 52 ... 256.09s passed\n",
      "Validation Accuracy = 0.964\n",
      "\n",
      "EPOCH 53 ... 260.89s passed\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "EPOCH 54 ... 265.72s passed\n",
      "Validation Accuracy = 0.945\n",
      "\n",
      "EPOCH 55 ... 270.52s passed\n",
      "Validation Accuracy = 0.945\n",
      "\n",
      "EPOCH 56 ... 275.31s passed\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "EPOCH 57 ... 280.11s passed\n",
      "Validation Accuracy = 0.943\n",
      "\n",
      "EPOCH 58 ... 285.08s passed\n",
      "Validation Accuracy = 0.935\n",
      "\n",
      "EPOCH 59 ... 289.94s passed\n",
      "Validation Accuracy = 0.963\n",
      "\n",
      "EPOCH 60 ... 294.81s passed\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "EPOCH 61 ... 299.63s passed\n",
      "Validation Accuracy = 0.963\n",
      "\n",
      "EPOCH 62 ... 304.47s passed\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "EPOCH 63 ... 309.25s passed\n",
      "Validation Accuracy = 0.966\n",
      "\n",
      "EPOCH 64 ... 314.06s passed\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "EPOCH 65 ... 318.83s passed\n",
      "Validation Accuracy = 0.929\n",
      "\n",
      "EPOCH 66 ... 323.64s passed\n",
      "Validation Accuracy = 0.935\n",
      "\n",
      "EPOCH 67 ... 328.34s passed\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "EPOCH 68 ... 333.01s passed\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "EPOCH 69 ... 337.79s passed\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "EPOCH 70 ... 342.51s passed\n",
      "Validation Accuracy = 0.963\n",
      "\n",
      "EPOCH 71 ... 347.36s passed\n",
      "Validation Accuracy = 0.961\n",
      "\n",
      "EPOCH 72 ... 352.14s passed\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "EPOCH 73 ... 356.92s passed\n",
      "Validation Accuracy = 0.968\n",
      "\n",
      "EPOCH 74 ... 361.67s passed\n",
      "Validation Accuracy = 0.944\n",
      "\n",
      "EPOCH 75 ... 366.43s passed\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "EPOCH 76 ... 371.27s passed\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "EPOCH 77 ... 376.03s passed\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "EPOCH 78 ... 380.76s passed\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "EPOCH 79 ... 385.59s passed\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "EPOCH 80 ... 390.41s passed\n",
      "Validation Accuracy = 0.967\n",
      "\n",
      "EPOCH 81 ... 395.16s passed\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "EPOCH 82 ... 399.90s passed\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "EPOCH 83 ... 404.70s passed\n",
      "Validation Accuracy = 0.971\n",
      "\n",
      "EPOCH 84 ... 409.41s passed\n",
      "Validation Accuracy = 0.967\n",
      "\n",
      "EPOCH 85 ... 414.14s passed\n",
      "Validation Accuracy = 0.970\n",
      "\n",
      "EPOCH 86 ... 418.91s passed\n",
      "Validation Accuracy = 0.972\n",
      "\n",
      "EPOCH 87 ... 423.72s passed\n",
      "Validation Accuracy = 0.969\n",
      "\n",
      "EPOCH 88 ... 428.47s passed\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "EPOCH 89 ... 433.22s passed\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "EPOCH 90 ... 437.92s passed\n",
      "Validation Accuracy = 0.970\n",
      "\n",
      "EPOCH 91 ... 442.69s passed\n",
      "Validation Accuracy = 0.974\n",
      "\n",
      "EPOCH 92 ... 447.49s passed\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "EPOCH 93 ... 452.33s passed\n",
      "Validation Accuracy = 0.961\n",
      "\n",
      "EPOCH 94 ... 457.07s passed\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "EPOCH 95 ... 461.88s passed\n",
      "Validation Accuracy = 0.938\n",
      "\n",
      "EPOCH 96 ... 466.64s passed\n",
      "Validation Accuracy = 0.937\n",
      "\n",
      "EPOCH 97 ... 471.46s passed\n",
      "Validation Accuracy = 0.971\n",
      "\n",
      "EPOCH 98 ... 476.27s passed\n",
      "Validation Accuracy = 0.970\n",
      "\n",
      "EPOCH 99 ... 481.09s passed\n",
      "Validation Accuracy = 0.969\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    starttime = time.time()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        \n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "\n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ... {:.2f}s passed\".format(i, time.time()-starttime))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    saver.save(sess, './vgg')\n",
    "    print(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./vgg\n",
      "Test Accuracy = 0.945\n"
     ]
    }
   ],
   "source": [
    "#Test set\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Output the Images / Top 5 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./vgg\n",
      "33 ['Turn right ahead']\n",
      "[ 2391.6159668    950.82513428   341.95428467    62.56286621  -337.75045776] ['Turn right ahead', 'Ahead only', 'Go straight or right', 'Turn left ahead', 'Keep left']\n",
      "35 ['Ahead only']\n",
      "[ 4620.69384766  1384.34118652  -160.41996765  -217.21606445  -420.67388916] ['Ahead only', 'Speed limit (60km/h)', 'Go straight or left', 'Turn left ahead', 'Turn right ahead']\n",
      "14 ['Stop']\n",
      "[ 1623.36010742   702.17297363   184.70770264    99.19914246    10.5238409 ] ['Stop', 'End of all speed and passing limits', 'No entry', 'Speed limit (60km/h)', 'No vehicles']\n",
      "25 ['Road work']\n",
      "[ 238.97966003  141.88952637   41.54112625  -29.61777878  -33.98166656] ['Road work', 'Stop', 'No vehicles', 'Speed limit (50km/h)', 'Yield']\n",
      "25 ['Road work']\n",
      "[ 1189.90087891   570.29284668    77.31018829   -19.93891144  -338.28860474] ['Road work', 'Wild animals crossing', 'Bumpy road', 'Bicycles crossing', 'Yield']\n"
     ]
    }
   ],
   "source": [
    "### Load the images and plot them here.\n",
    "#from fnmatch import fnmatch\n",
    "#import os\n",
    "import matplotlib.image as mpimg\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "#jpegs = [f for f in os.listdir(\"test_images/\") if fnmatch(f, \"*.jpg\")]\n",
    "width, height = 32, 32\n",
    "\n",
    "# read csv as dict\n",
    "df_signnames = pd.read_csv('./signnames.csv')\n",
    "\n",
    "test_images = np.zeros((5,32,32,3))\n",
    "for i in range(5):\n",
    "    im = Image.open(\"test_images/\"+str(i)+\".jpg\").resize((32,32), Image.ANTIALIAS)\n",
    "    test_images[i] = np.array(im)\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    feed_dict = {x: test_images, y:[33, 14, 35, 25, 8], keep_prob: 1}\n",
    "    classification = sess.run(tf.argmax(logits,1), feed_dict=feed_dict)\n",
    "    prob = sess.run(logits, feed_dict=feed_dict)\n",
    "    top5 = sess.run(tf.nn.top_k(tf.constant(prob), k=5))\n",
    "    for i in range(5):\n",
    "        print(classification[i], df_signnames[df_signnames.ClassId==classification[i]].SignName.values)\n",
    "        \n",
    "        top5labels = []\n",
    "        for j in range(5):\n",
    "            top5labels.append(df_signnames[df_signnames.ClassId==top5.indices[i][j]].SignName.values.item(0))\n",
    "        \n",
    "        print(top5.values[i], top5labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4 (Optional): Visualize the Neural Network's State with Test Images\n",
    "\n",
    " This Section is not required to complete but acts as an additional excersise for understaning the output of a neural network's weights. While neural networks can be a great learning device they are often referred to as a black box. We can understand what the weights of a neural network look like better by plotting their feature maps. After successfully training your neural network you can see what it's feature maps look like by plotting the output of the network's weight layers in response to a test stimuli image. From these plotted feature maps, it's possible to see what characteristics of an image the network finds interesting. For a sign, maybe the inner network feature maps react with high activation to the sign's boundary outline or to the contrast in the sign's painted symbol.\n",
    "\n",
    " Provided for you below is the function code that allows you to get the visualization output of any tensorflow weight layer you want. The inputs to the function should be a stimuli image, one used during training or a new one you provided, and then the tensorflow variable name that represents the layer's state during the training process, for instance if you wanted to see what the [LeNet lab's](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) feature maps looked like for it's second convolutional layer you could enter conv2 as the tf_activation variable.\n",
    "\n",
    "For an example of what feature map outputs look like, check out NVIDIA's results in their paper [End-to-End Deep Learning for Self-Driving Cars](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/) in the section Visualization of internal CNN State. NVIDIA was able to show that their network's inner weights had high activations to road boundary lines by comparing feature maps from an image with a clear path to one without. Try experimenting with a similar test to show that your trained network's weights are looking for interesting features, whether it's looking at differences in feature maps from images with or without a sign, or even what feature maps look like in a trained network vs a completely untrained one on the same sign image.\n",
    "\n",
    "<figure>\n",
    " <img src=\"visualize_cnn.png\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output should look something like this (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize your network's feature maps here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
